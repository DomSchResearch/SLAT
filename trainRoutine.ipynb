{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SLAT\n",
    "\n",
    "This is the routine for training SLAT on a given dataset. For logging the model performance W&B is used and for simplified training Lightning is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary modules\n",
    "\n",
    "We will use W&B logger as Lightning integration. The Trainer will be loaded from Lightning. The dataset is available as \".mat\" file and will be loaded using the scipy.io package, then reshaped and finally converted into a TensorDataset. For automatic batch creation the DataLoader class is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch import Trainer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import SLAT\n",
    "import wandb\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Makros\n",
    "\n",
    "Here, we define our api tokens for W&B and HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_API_KEY = \"\"\n",
    "HF_KEY = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login\n",
    "\n",
    "In this section we log in to W&B and HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=WANDB_API_KEY)\n",
    "huggingface_hub.login(token=HF_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data loaders\n",
    "\n",
    "The datasets are loaded and prepared to be used in a training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeWindowSize = 40\n",
    "features = 34\n",
    "\n",
    "X_train = sio.loadmat('./data/trainX_new.mat')\n",
    "X_train = X_train['train1X_new']\n",
    "X_train = X_train.reshape(len(X_train), timeWindowSize+2, features)\n",
    "Y_train = sio.loadmat('./data/trainY.mat')\n",
    "Y_train = Y_train['train1Y']\n",
    "Y_train = Y_train.transpose()\n",
    "\n",
    "X_test = sio.loadmat('./data/testX_new.mat')\n",
    "X_test = X_test['test1X_new']\n",
    "X_test = X_test.reshape(len(X_test), timeWindowSize+2, features)\n",
    "Y_test = sio.loadmat('./data/testY.mat')\n",
    "Y_test = Y_test['test1Y']\n",
    "Y_test = Y_test.transpose()\n",
    "\n",
    "training_set = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float),\n",
    "    torch.tensor(Y_train, dtype=torch.float)\n",
    ")\n",
    "validation_set = TensorDataset(\n",
    "    torch.tensor(X_test, dtype=torch.float),\n",
    "    torch.tensor(Y_test, dtype=torch.float)\n",
    ")\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    training_set,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    validation_set,\n",
    "    batch_size=256,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In the first stage, the non-trained model is loaded. Second, the checkpoint is defined and the variable that is monitored. Third, the logger from W&B is instanciated. Afterwards, the trainier itself is instanciated and the model is fitted on the data. Then, the finishing of the logger is called and finally the trained model is pushed to HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SLAT.SLAT_LitModule()\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_RMSE', mode='min')\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='SLAT',\n",
    "    log_model='all'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=300\n",
    ")\n",
    "\n",
    "trainer.fit(model, training_loader, validation_loader)\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "model.push_to_hub(\n",
    "    \"dschneider96/SLAT\",\n",
    "    use_auth_token=True,\n",
    "    commit_message=\"basic training\",\n",
    "    private=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "This is a basic example for prediction using the pretrained model from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained = SLAT.SLAT_LitModule.from_pretrained(\"dschneider96/SLAT\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\"\n",
    ")\n",
    "\n",
    "test_loader = validation_loader\n",
    "predictions = trainer.predict(model=model_pretrained, dataloaders=test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
